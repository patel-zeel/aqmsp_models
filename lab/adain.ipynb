{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from einops import rearrange, repeat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([9, 13, 1])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([9, 13, 1])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, n_spatial, n_spatio_temporal, dropout):\n",
    "        super().__init__()\n",
    "        self.act = nn.ReLU()\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        self.input_fc = nn.Linear(n_spatial, 100)\n",
    "        self.input_lstm = nn.LSTM(n_spatio_temporal, 300, num_layers=2, batch_first=True, dropout=self.dropout)\n",
    "        \n",
    "        self.fc2 = nn.Linear(400, 200)\n",
    "        \n",
    "    def forward(self, x_spatial, x_spatio_temporal, y_spatio_temporal):  # x_spatial: (batch_size, n_context_stations, n_spatial), x_spatio_temporal: (batch_size, n_context_stations, window_size, n_spatio_temporal), y_spatio_temporal: (batch_size, n_context_stations, window_size, 1)\n",
    "        batch_size = x_spatial.shape[0]\n",
    "        xy_spatio_temporal = torch.cat([x_spatio_temporal, y_spatio_temporal], dim=-1)\n",
    "        \n",
    "        z_spatial = torch.vmap(lambda x: F.dropout(self.act(self.input_fc(x)), p=self.dropout), randomness=\"same\")(x_spatial)\n",
    "        \n",
    "        xy_spatio_temporal = rearrange(xy_spatio_temporal, 'batch_size n_context_stations window_size n_spatio_temporal -> (batch_size n_context_stations) window_size n_spatio_temporal')\n",
    "        z_spatio_temporal, _ = self.input_lstm(xy_spatio_temporal)\n",
    "        z_spatio_temporal = rearrange(z_spatio_temporal[:, -1, :], '(batch_size n_context_stations) lstm_out -> batch_size n_context_stations lstm_out', batch_size=batch_size)\n",
    "        \n",
    "        z_concat = torch.cat([z_spatial, z_spatio_temporal], dim=-1)\n",
    "        z_concat = torch.vmap(lambda x: F.dropout(self.act(self.fc2(x)), p=self.dropout), randomness=\"same\")(z_concat)\n",
    "        \n",
    "        return z_concat\n",
    "    \n",
    "class decoder(nn.Module):\n",
    "    def __init__(self, n_spatial, n_spatio_temporal, dropout):\n",
    "        super().__init__()\n",
    "        self.act = nn.ReLU()\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        self.input_fc = nn.Linear(n_spatial, 100)\n",
    "        self.input_lstm = nn.LSTM(n_spatio_temporal, 300, num_layers=2, batch_first=True, dropout=self.dropout)\n",
    "        \n",
    "        self.fc2 = nn.Linear(400, 200)\n",
    "        \n",
    "    def forward(self, x_spatial, x_spatio_temporal):  # x_spatial: (batch_size, n_target_stations, n_spatial), x_spatio_temporal: (batch_size, n_target_stations, window_size, n_spatio_temporal), y_spatio_temporal: (batch_size, n_target_stations, window_size, 1)\n",
    "        # We don't use vmap out of the box because it doesn't support the LSTM layer.\n",
    "        batch_size = x_spatial.shape[0]\n",
    "        \n",
    "        z_spatial = torch.vmap(lambda x: F.dropout(self.act(self.input_fc(x)), p=self.dropout), randomness=\"same\")(x_spatial)\n",
    "        \n",
    "        x_spatio_temporal = rearrange(x_spatio_temporal, 'batch_size n_target_stations window_size n_spatio_temporal -> (batch_size n_target_stations) window_size n_spatio_temporal')\n",
    "        z_spatio_temporal, _ = self.input_lstm(x_spatio_temporal)\n",
    "        z_spatio_temporal = rearrange(z_spatio_temporal[:, -1, :], '(batch_size n_target_stations) lstm_out -> batch_size n_target_stations lstm_out', batch_size=batch_size)\n",
    "        \n",
    "        z_concat = torch.cat([z_spatial, z_spatio_temporal], dim=-1)\n",
    "        z_concat = torch.vmap(lambda x: F.dropout(self.act(self.fc2(x)), p=self.dropout), randomness=\"same\")(z_concat)\n",
    "        \n",
    "        return z_concat\n",
    "\n",
    "class AttentionNet(nn.Module):\n",
    "    def __init__(self, dropout):\n",
    "        super().__init__()\n",
    "        self.dropout = dropout\n",
    "        self.fc = nn.Linear(400, 200)\n",
    "        self.fc2 = nn.Linear(200, 1)\n",
    "        \n",
    "    def forward(self, z_context, z_target):  # z_context: (batch_size, num_context_stations, 200), z_target: (batch_size, num_target_stations, 200)\n",
    "        num_context_stations = z_context.shape[1]\n",
    "        \n",
    "        def single_forward(z_target, z_context):\n",
    "            z_target = repeat(z_target, \"z_dim -> num_content_stations z_dim\", num_content_stations=num_context_stations)\n",
    "            z = torch.cat([z_target, z_context], dim=-1) # (num_context_stations, 200) + (num_context_stations, 200) = (num_context_stations, 400)\n",
    "            z = F.dropout(F.relu(self.fc(z)), p=self.dropout) # (num_context_stations, 400) -> (num_context_stations, 200)\n",
    "            z = self.fc2(z) # (num_context_stations, 200) -> (num_context_stations, 1)\n",
    "            z = F.softmax(z, dim=0)  # (num_context_stations, 1)\n",
    "            return rearrange(z, 'num_context_stations 1 -> num_context_stations')\n",
    "        \n",
    "        # (num_context_stations, 200) + (num_target_stations, 200) -> (num_context_stations, num_target_stations)\n",
    "        multi_forward = torch.vmap(single_forward, in_dims=(0, None), out_dims=1, randomness=\"same\")\n",
    "        # (batch_size, num_context_stations, 200) + (batch_size, num_target_stations, 200) -> (batch_size, num_context_stations, num_target_stations)\n",
    "        attention = torch.vmap(multi_forward, randomness=\"same\")(z_target, z_context)\n",
    "        return attention\n",
    "    \n",
    "class ADAIN(nn.Module):\n",
    "    def __init__(self, n_spatial, n_spatio_temporal, dropout):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(n_spatial, n_spatio_temporal, dropout)\n",
    "        self.decoder = decoder(n_spatial, n_spatio_temporal, dropout)\n",
    "        self.attention = AttentionNet(dropout)\n",
    "        self.fc = nn.Linear(200, 1)\n",
    "        \n",
    "    def forward(self, x_context_spatial, x_context_spatio_temporal, y_context_spatio_temporal, x_target_spatial, x_target_spatio_temporal):\n",
    "        z_context = self.encoder(x_context_spatial, x_context_spatio_temporal, y_context_spatio_temporal) # (batch_size, num_context_stations, 200)\n",
    "        z_target = self.decoder(x_target_spatial, x_target_spatio_temporal) # (batch_size, num_target_stations, 200)\n",
    "        attention = self.attention(z_context, z_target) # (batch_size, num_context_stations, num_target_stations)\n",
    "        def get_output(attention):\n",
    "            attention = rearrange(attention, 'batch_size num_context_stations -> batch_size num_context_stations 1')\n",
    "            output =  attention * z_context # (batch_size, num_context_stations, 200)\n",
    "            output = torch.sum(output, dim=1) # (batch_size, 200)\n",
    "            output = self.fc(output) # (batch_size, 1)\n",
    "            return output\n",
    "            \n",
    "        # (batch_size, num_context_stations, num_target_stations) \n",
    "        output = torch.vmap(get_output, in_dims=2, out_dims=1)(attention)\n",
    "        print(output.shape)\n",
    "        return output\n",
    "        \n",
    "batch_size = 9 # over time\n",
    "n_spatial = 7\n",
    "n_spatio_temporal = 11\n",
    "n_context_stations = 5\n",
    "n_target_stations = 13\n",
    "window_size = 24\n",
    "\n",
    "x_context_spatial = torch.randn(batch_size, n_context_stations, n_spatial)\n",
    "x_context_spatio_temporal = torch.randn(batch_size, n_context_stations, window_size, n_spatio_temporal)\n",
    "y_context_spatio_temporal = torch.randn(batch_size, n_context_stations, window_size, 1)\n",
    "x_target_spatial = torch.randn(batch_size, n_target_stations, n_spatial)\n",
    "x_target_spatio_temporal = torch.randn(batch_size, n_target_stations, window_size, n_spatio_temporal)\n",
    "\n",
    "model = ADAIN(n_spatial, n_spatio_temporal, 0.01)\n",
    "model(x_context_spatial, x_context_spatio_temporal, y_context_spatio_temporal, x_target_spatial, x_target_spatio_temporal).shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_gpu_py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
